{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr size=7 color=#8D84B5 > </hr> \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "# <font color = #6b4cde face=\"Verdana\"> **Universities and Gentrification**\n",
    "## <font color = #6b4cde face=\"Verdana\"> **UMD CMSC320 Data Science, Spring 2023** </font>\n",
    "## <font color = #6b4cde face=\"Verdana\"> **Joe Diaz and Connor Pymm** </font>\n",
    "</center>\n",
    "\n",
    "</div>\n",
    "\n",
    "<hr size=7 color=#8D84B5 > </hr> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üôèRUN ME FIRSTüôè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr size=7 color=#8D84B5 > </hr> \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "## <font color = #6b4cde face=\"Verdana\"> **Data Curation** </font>\n",
    "</center>\n",
    "\n",
    "</div>\n",
    "\n",
    "<hr size=7 color=#8D84B5 > </hr> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Datasets\n",
    "\n",
    "In order to perform analysis on colleges and their surrounding regions, we needed to\n",
    "find some subset of colleges, a dataset with characteristics of those colleges on a yearly basis, \n",
    "and then a dataset with characteristics of their nearby geographical areas. again yearly. \n",
    "\n",
    "Initially, we decided to limit our analysis to the top 100 universities in the country \n",
    "according to current US News rankings, under the assumption that more highly ranked universities \n",
    "might have a more significant impact on their respective communities. We used Andy Reiter's\n",
    "‚ÄúU.S. News & World Report Historical Liberal Arts College and University Rankings‚Äù dataset (**citation**).\n",
    "  \n",
    "In order to obtain college characteristics, we discovered that the Department of\n",
    "Education has extensive data available on accredited universities called the College Scorecard, which has\n",
    "a public API for programatically querying data.\n",
    "  \n",
    "In order to obtain characteristics of the region around each university, we needed a dataset that would contain\n",
    "demographic and economic data for defined geographical regions associated with the location of the University.\n",
    "We found that the American Community Survey yearly data from the Census had the housing cost and income data we\n",
    "wanted to analyze, and that its Public Use Microdata API from the census allowed us to programatically request that\n",
    "data for geographical groupings called \"Public Use Microdata Areas,\" which are the smallest geographical entities that\n",
    "the Census collects yearly data from."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract, Transform, and Load\n",
    "\n",
    "Since we queried a *substantial* amount of data from *ridiculously large* datasets,\n",
    "and requesting federal data from the Department of Education and the Census required\n",
    "registration for and usage of API keys, we decided that on top of the source datasets that\n",
    "we were able to download in full, stored in our repository under ETL/source_data, we would\n",
    "create modules for making federal API requests and loading the results into csv files for usage\n",
    "later. \n",
    "  \n",
    "Dataframes that we generated from data that we queried were stored under ETL/generated_data\n",
    "as csv, and then loaded into the notebook when needed, specifically: we built ScorecardData.csv using\n",
    "our scorecard_client.py module, which defines a CollegeScorecardClient object that can be used to query\n",
    "DoE data, given a valid API key, set of desired variables, and set of colleges using IPEDS IDs, we built \n",
    "college_FIPs by combining the university list we got from Reiter with state FIPs data from DoE and county \n",
    "FIPs data by collecting them manually university by university.\n",
    "\n",
    "For the rest of this tutorial, we will be using the data we collected by default, but if you would like to\n",
    "recreate the analysis of this tutorial using a different set of colleges, and thus your own datasets, you can\n",
    "fork this repository and use the modules provided in the ETL/ directory to do so."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr size=7 color=#8D84B5 > </hr> \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "## <font color = #6b4cde face=\"Verdana\"> **Data Processing** </font>\n",
    "</center>\n",
    "\n",
    "</div>\n",
    "\n",
    "<hr size=7 color=#8D84B5 > </hr> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Representation\n",
    "\n",
    "Here, we load the data we have downloaded or generated locally into our\n",
    "notebook for use to use in our analysis. We stored each of our datasets as\n",
    "csv, so they are easily loaded into Pandas Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataframes from Scorecard generated data\n",
    "scard_df = pd.read_csv(\"../ETL/generated_data/ScorecardData.csv\")\n",
    "fips_df = pd.read_csv(\"../ETL/generated_data/college_FIPs.csv\")\n",
    "cpi_df = pd.read_csv(\"../ETL/source_data/cpi_all.csv\").groupby(\"Year\")[\"Value\"].mean()\n",
    "scard_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2009, 2020)\n",
    "msa_path_format = \"../ETL/generated_data/MSA{y}.csv\"\n",
    "MSA_frames = [\n",
    "    pd.read_csv(msa_path_format.format(y=yr)).assign(year=yr)\n",
    "    for yr in years\n",
    "]\n",
    "msa_df = pd.concat(MSA_frames)\n",
    "msa_df.columns = [\"name\", \"msa_income\", \"msa_rent\", \"msa\", \"year\"]\n",
    "msa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2009, 2020)\n",
    "puma_path_format = \"../ETL/generated_data/PUMA{y}.csv\"\n",
    "PUMA_frames = [\n",
    "    pd.read_csv(puma_path_format.format(y=yr)).assign(year=yr)\n",
    "    for yr in years\n",
    "]\n",
    "puma_df = pd.concat(PUMA_frames)\n",
    "puma_df.columns = [\"puma_income\", \"puma_rent\", \"state\", \"puma\", \"year\"]\n",
    "puma_df[\"state\"] = puma_df[\"state\"].astype(\"int\")\n",
    "puma_df[\"puma\"] = puma_df[\"puma\"].astype(\"int\")\n",
    "puma_df[\"year\"] = puma_df[\"year\"].astype(\"int\")\n",
    "\n",
    "print(puma_df[\"puma\"].unique().shape)\n",
    "puma_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Reshaping\n",
    "\n",
    "The data that we have still uses the variable names and formatting of our\n",
    "original sources, and those variable names are unweildy and not ideal for usage\n",
    "in analysis later, so we rename our columns to be more human readable and\n",
    "developer friendly. Additionally, cost data in our sources does not account for\n",
    "inflation, so we should use an all-consumers/all-goods CPI to transform our dollar\n",
    "values to a standard value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to be more readable, usable\n",
    "scard_df = scard_df.rename(\n",
    "    columns={\n",
    "        \"student.size\": \"size\",\n",
    "        \"cost.tuition.in_state\": \"in_state_tuition\",\n",
    "        \"cost.tuition.out_of_state\": \"out_state_tuition\",\n",
    "        \"cost.avg_net_price.public\": \"public_net_price\",\n",
    "        \"cost.avg_net_price.private\": \"private_net_price\",\n",
    "        \"id\": \"ipeds_id\",\n",
    "        \"school.name\": \"name\",\n",
    "        \"school.carnegie_size_setting\": \"size_setting\",\n",
    "        \"school.zip\": \"zip\",\n",
    "        \"school.state_fips\": \"state_fips\",\n",
    "        \"school.region_id\": \"region_id\",\n",
    "        \"school.locale\": \"locale\",\n",
    "        \"school.ownership\": \"ownership\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Join county FIPs codes into College Scorecard dataframe for use later in\n",
    "# associating with Census geographies.\n",
    "scard_df = pd.merge(scard_df, fips_df[[\"name\", \"county\", \"cbsa\", \"puma\"]], on=\"name\", how=\"left\").drop_duplicates()\n",
    "\n",
    "\n",
    "# Combine public and private net prices into a single net price column, and drop those columns\n",
    "scard_df[\"net_cost\"] = scard_df.apply(lambda row: \n",
    "            row[\"public_net_price\"] if (row[\"ownership\"] == 1) else row[\"private_net_price\"],\n",
    "        axis=1\n",
    ")\n",
    "scard_df[\"net_cost_adjusted\"] = scard_df.apply(lambda row: \n",
    "            (row[\"net_cost\"]/cpi_df.at[row[\"year\"]]) * 100,\n",
    "        axis=1\n",
    ")\n",
    "scard_df[\"in_tuition_adjusted\"] = scard_df.apply(lambda row: \n",
    "            (row[\"in_state_tuition\"]/cpi_df.at[row[\"year\"]]) * 100,\n",
    "        axis=1\n",
    ")\n",
    "scard_df[\"out_tuition_adjusted\"] = scard_df.apply(lambda row: \n",
    "            (row[\"out_state_tuition\"]/cpi_df.at[row[\"year\"]]) * 100,\n",
    "        axis=1\n",
    ")\n",
    "scard_df[\"state_fips\"] = scard_df[\"state_fips\"].astype(int)\n",
    "scard_df[\"year\"] = scard_df[\"year\"].astype(int)\n",
    "scard_df.drop([\"public_net_price\", \"private_net_price\"], axis=1, inplace=True)\n",
    "scard_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note that some rows do not have cost data associated with them, thus they are missing data.\n",
    "Since we will use this cost data later in our analysis, we need to either interpolate the missing data\n",
    "or drop the invalid rows. Here, we experiment with dropping rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scard_clipped = scard_df.dropna(subset=[\"net_cost\", \"in_state_tuition\", \"out_state_tuition\"]).copy()\n",
    "print(scard_clipped[\"name\"].unique().shape)\n",
    "print(scard_clipped.to_string())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems as if the clipped dataframe after dropping null cost data is just the data after 2009.\n",
    "To verify that this is true, I try querying the original dataset purely by restricting the years.\n",
    "If there is complete cost data from 2009 to 2020, then the resulting dataframe should be equal to the\n",
    "dataframe resulting from dropping null data. Run the next code cell to confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scard_clipped_year = scard_df[scard_df[\"year\"] >= 2009].copy()\n",
    "scard_clipped_year.equals(scard_clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_df[\"msa\"] = msa_df[\"msa\"].str.replace(\"]\", \"\").astype(int)\n",
    "msa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scard_clipped_geo = scard_clipped.dropna(subset=[\"cbsa\", \"puma\"]).copy()\n",
    "scard_clipped_geo[\"puma\"] = scard_clipped_geo[\"puma\"].astype(int)\n",
    "scard_clipped_geo[\"cbsa\"] = scard_clipped_geo[\"cbsa\"].astype(int)\n",
    "print(scard_clipped_geo[\"puma\"].unique().shape)\n",
    "scard_clipped_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_uni_list = scard_df[\"name\"].unique().tolist()\n",
    "mapped_uni_list = fips_df[\"name\"].unique().tolist()\n",
    "clipped_uni_list = scard_clipped_geo[\"name\"].unique().tolist()\n",
    "\n",
    "cul_set = set(clipped_uni_list)\n",
    "uni_diff1 = [uni for uni in full_uni_list if uni not in cul_set]\n",
    "uni_diff2 = [uni for uni in mapped_uni_list if uni not in cul_set]\n",
    "\n",
    "scard_df = scard_clipped_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Violin(\n",
    "        x=scard_clipped['year'], \n",
    "        y=scard_clipped['net_cost'],\n",
    "        box_visible=True,\n",
    "        meanline_visible=True\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Violin(\n",
    "        x=scard_clipped['year'], \n",
    "        y=scard_clipped['in_state_tuition'],\n",
    "        box_visible=True,\n",
    "        meanline_visible=True\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Violin(\n",
    "        x=scard_clipped['year'], \n",
    "        y=scard_clipped['out_state_tuition'],\n",
    "        box_visible=True,\n",
    "        meanline_visible=True\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(scard_clipped, x=\"year\", y=\"in_state_tuition\", facet_col=\"ownership\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(scard_clipped, x=\"year\", y=\"in_tuition_adjusted\", facet_col=\"ownership\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(scard_clipped, x=\"year\", y=\"out_tuition_adjusted\", facet_col=\"ownership\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scard_df = pd.merge(scard_df, puma_df, left_on=[\"state_fips\", \"puma\", \"year\"], right_on=[\"state\", \"puma\", \"year\"], how=\"left\")\n",
    "scard_df[\"name\"].unique().shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
